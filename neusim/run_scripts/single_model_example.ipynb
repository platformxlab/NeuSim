{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda4f587",
   "metadata": {},
   "source": [
    "## Running a Single Model Simulation\n",
    "\n",
    "The `neusim/npusim/frontend` directory contains the ops generator classes that acts as the frontend of the NPU simulator.\n",
    "Each ops generator represents the DNN graph of a specific model architecture, such as LLM, DLRM, and stable diffusion models.\n",
    "\n",
    "Each ops generator class can be invoked directly in a Python script by feeding it a simulation configuration. It can generate the operators of a fordward or backward pass of a DNN model and dump the operator-level performance stats into a CSV file. Here, we provide a minimum working example, which launches the simulation for Llama3-8B inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0ca05",
   "metadata": {},
   "source": [
    "### Creating the Simulation Configuration\n",
    "\n",
    "First, we need to create a simulation configuration. This can be either a Python dictionary or an `LLMConfig` object (see [`LLMConfig.py`](../configs/models/LLMConfig.py) for all configurable parameters).\n",
    "It is recommended to put all configuration parameters into JSON files (see [`configs/`](../../configs/) for examples; most of the parameters have self-explanatory names), load and merging them into a Python dictionary, and then create the `LLMConfig` object from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe0061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuSim root directory: /mnt/nvme0n1p1/yuqixue2/neusim/NeuSim\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from neusim.configs.models.LLMConfig import LLMConfig\n",
    "\n",
    "neusim_root = Path(\"./\").resolve().parent.parent\n",
    "print(f\"NeuSim root directory: {neusim_root}\")\n",
    "\n",
    "# Load model configuration from JSON file\n",
    "with open(neusim_root / \"configs/models/llama3-8b.json\") as f:\n",
    "    model_cfg = json.load(f)\n",
    "\n",
    "# Load NPU configuration from JSON file\n",
    "with open(neusim_root / \"configs/chips/tpuv5p.json\") as f:\n",
    "    npu_cfg = json.load(f)\n",
    "\n",
    "# load system configuration from JSON file\n",
    "with open(neusim_root / \"configs/systems/system_config.json\") as f:\n",
    "    system_cfg = json.load(f)\n",
    "\n",
    "# Merge all configurations into a single dictionary\n",
    "config_dict = { **system_cfg, **npu_cfg, **model_cfg }\n",
    "\n",
    "# If you want to override some parameters, you can do so by directly modifying the dictionary.\n",
    "# For example, to specify the output file path:\n",
    "config_dict[\"output_file_path\"] = str(neusim_root / \"results/single_model_run/llama3-8b-inference-v5p.csv\")\n",
    "\n",
    "# create output directory if it does not exist\n",
    "output_dir = Path(config_dict[\"output_file_path\"]).parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create an LLMConfig object from the dictionary\n",
    "# This step is optional as our ops generator class can accept a Python dictionary directly\n",
    "# and automatically convert it to an `LLMConfig` object internally.\n",
    "config: LLMConfig = LLMConfig.model_validate(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6bf78",
   "metadata": {},
   "source": [
    "We can change the parameters to simulate different parallelism configurations, sequence lengths, batch sizes, etc., as shown in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a5e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_chips = 16  # total number of NPUs in the system\n",
    "config.tensor_parallelism_degree = 4\n",
    "config.pipeline_parallelism_degree = 4\n",
    "config.input_seqlen = 8192\n",
    "config.output_seqlen = 1024\n",
    "config.global_batch_size = 16\n",
    "config.microbatch_size_ici = config.global_batch_size // config.pipeline_parallelism_degree  # micro-batch size per pipeline stage; set to global_batch_size if no pipeline parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28924be6",
   "metadata": {},
   "source": [
    "### Running the Simulation\n",
    "\n",
    "Then, we create the ops generator instance and invoke the `generate()` function to trigger the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4d73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated vmem_time_ns: 0 for op: PrefillReceiveInputFromPipelineICIreceiveInputICITransfer33554432\n",
      "Op: PrefillReceiveInputFromPipelineICIreceiveInputICITransfer33554432, mxu_time: 0, vpu_time: 0, compute_time: 0, memory_time: 22604, ici_time: 335545, vmem_time: 0, exe_time: 335545\n",
      "Calculated vmem_time_ns: 9417.172296262255 for op: FwdAttentionencoderInputlayernormXnormLayerNormX\n",
      "Op: FwdAttentionencoderInputlayernormXnormLayerNormX, mxu_time: 0, vpu_time: 25701, compute_time: 25701, memory_time: 45208, ici_time: 0, vmem_time: 9417.172296262255, exe_time: 45208\n",
      "Calculated vmem_time_ns: 426169.11148131127 for op: Q4MatMulQ\n",
      "Op: Q4MatMulQ, mxu_time: 2467257, vpu_time: 411206, compute_time: 2467257, memory_time: 226040, ici_time: 0, vmem_time: 426169.11148131127, exe_time: 2467257\n",
      "Calculated vmem_time_ns: 426169.11148131127 for op: K4MatMulK\n",
      "Op: K4MatMulK, mxu_time: 2467257, vpu_time: 411206, compute_time: 2467257, memory_time: 226040, ici_time: 0, vmem_time: 426169.11148131127, exe_time: 2467257\n",
      "Calculated vmem_time_ns: 426169.11148131127 for op: V4MatMulV\n",
      "Op: V4MatMulV, mxu_time: 2467257, vpu_time: 411206, compute_time: 2467257, memory_time: 226040, ici_time: 0, vmem_time: 426169.11148131127, exe_time: 2467257\n",
      "Calculated vmem_time_ns: 1668767.0450367648 for op: FlashAttention5FlashAttention\n",
      "Op: FlashAttention5FlashAttention, mxu_time: 9868990, vpu_time: 4934474, compute_time: 9868990, memory_time: 361664, ici_time: 0, vmem_time: 1668767.0450367648, exe_time: 9868990\n",
      "Calculated vmem_time_ns: 426169.11148131127 for op: Attentionoutput6MatMulattnOutputattnAvgWo\n",
      "Op: Attentionoutput6MatMulattnOutputattnAvgWo, mxu_time: 2467257, vpu_time: 411206, compute_time: 2467257, memory_time: 226040, ici_time: 0, vmem_time: 426169.11148131127, exe_time: 2467257\n",
      "Calculated vmem_time_ns: 37668.18918504902 for op: Attentionlayernorm7YnormLayerNormy\n",
      "Op: Attentionlayernorm7YnormLayerNormy, mxu_time: 0, vpu_time: 102802, compute_time: 102802, memory_time: 180832, ici_time: 0, vmem_time: 37668.18918504902, exe_time: 180832\n",
      "Calculated vmem_time_ns: 1482608.0399624694 for op: FwdFFNencoderFFgateMatMulhgate2ynorm2WFFgate1\n",
      "Op: FwdFFNencoderFFgateMatMulhgate2ynorm2WFFgate1, mxu_time: 8635351, vpu_time: 1439222, compute_time: 8635351, memory_time: 655516, ici_time: 0, vmem_time: 1482608.0399624694, exe_time: 8635351\n",
      "Calculated vmem_time_ns: 1482608.0399624694 for op: FwdFFNencoderFFupMatMulhup2ynorm2WFFup1\n",
      "Op: FwdFFNencoderFFupMatMulhup2ynorm2WFFup1, mxu_time: 8635351, vpu_time: 1439222, compute_time: 8635351, memory_time: 655516, ici_time: 0, vmem_time: 1482608.0399624694, exe_time: 8635351\n",
      "Calculated vmem_time_ns: 53130.91214767157 for op: FwdFFNencoderFFgateuphgateup2hgate2hup1\n",
      "Op: FwdFFNencoderFFgateuphgateup2hgate2hup1, mxu_time: 0, vpu_time: 44976, compute_time: 44976, memory_time: 632912, ici_time: 0, vmem_time: 53130.91214767157, exe_time: 632912\n",
      "Calculated vmem_time_ns: 1476624.195369945 for op: FwdFFNencoderFFoutputMatMulffdown2hgateup2WFFdown1\n",
      "Op: FwdFFNencoderFFoutputMatMulffdown2hgateup2WFFdown1, mxu_time: 8635351, vpu_time: 1439222, compute_time: 8635351, memory_time: 565100, ici_time: 0, vmem_time: 1476624.195369945, exe_time: 8635351\n",
      "Calculated vmem_time_ns: 15180.43918504902 for op: FwdFFNencoderAttnPlusFFnattnPlusFFnAddBdLlMmhBdLlMmh\n",
      "Op: FwdFFNencoderAttnPlusFFnattnPlusFFnAddBdLlMmhBdLlMmh, mxu_time: 0, vpu_time: 12851, compute_time: 12851, memory_time: 180832, ici_time: 0, vmem_time: 15180.43918504902, exe_time: 180832\n",
      "Calculated vmem_time_ns: 0 for op: PrefillSendOutputToPipelineICISendOutputICITransfer33554432\n",
      "Op: PrefillSendOutputToPipelineICISendOutputICITransfer33554432, mxu_time: 0, vpu_time: 0, compute_time: 0, memory_time: 22604, ici_time: 335545, vmem_time: 0, exe_time: 335545\n",
      "Calculated vmem_time_ns: 0 for op: DecodeReceiveInputFromPipelineICIreceiveInputICITransfer4096\n",
      "Op: DecodeReceiveInputFromPipelineICIreceiveInputICITransfer4096, mxu_time: 0, vpu_time: 0, compute_time: 0, memory_time: 500, ici_time: 3330, vmem_time: 0, exe_time: 3330\n",
      "Calculated vmem_time_ns: 34.090628829656865 for op: AttentionservingdecodeInputlayernormXnormLayerNormX\n",
      "Op: AttentionservingdecodeInputlayernormXnormLayerNormX, mxu_time: 0, vpu_time: 4, compute_time: 4, memory_time: 500, ici_time: 0, vmem_time: 34.090628829656865, exe_time: 500\n",
      "Calculated vmem_time_ns: 2359.0027429917277 for op: AttentionservingdecodeQKVMatMulQ2xnormallgathered2Wq1\n",
      "Op: AttentionservingdecodeQKVMatMulQ2xnormallgathered2Wq1, mxu_time: 9657, vpu_time: 1606, compute_time: 9657, memory_time: 11325, ici_time: 0, vmem_time: 2359.0027429917277, exe_time: 11325\n",
      "Calculated vmem_time_ns: 4714.044644224878 for op: AttentionservingdecodeQKVMatMulKV2xnorm2Wkv1\n",
      "Op: AttentionservingdecodeQKVMatMulKV2xnorm2Wkv1, mxu_time: 19295, vpu_time: 3212, compute_time: 19295, memory_time: 22638, ici_time: 0, vmem_time: 4714.044644224878, exe_time: 22638\n",
      "Calculated vmem_time_ns: 18884.696068857233 for op: AttentionservingdecodeSoftmaxQKVMatMulQKprefix2Q2Kcache2\n",
      "Op: AttentionservingdecodeSoftmaxQKVMatMulQKprefix2Q2Kcache2, mxu_time: 77120, vpu_time: 12850, compute_time: 77120, memory_time: 91134, ici_time: 0, vmem_time: 18884.696068857233, exe_time: 91134\n",
      "Calculated vmem_time_ns: 2364.098699831495 for op: AttentionservingdecodeSoftmaxQKVMatMulQKsuffix2Q2Ksuffix2\n",
      "Op: AttentionservingdecodeSoftmaxQKVMatMulQKsuffix2Q2Ksuffix2, mxu_time: 9657, vpu_time: 1606, compute_time: 9657, memory_time: 11402, ici_time: 0, vmem_time: 2364.098699831495, exe_time: 11402\n",
      "Calculated vmem_time_ns: 218.22819967830884 for op: AttentionservingdecodeSoftmaxQKVattnWeightsSoftmaxattnWeights\n",
      "Op: AttentionservingdecodeSoftmaxQKVattnWeightsSoftmaxattnWeights, mxu_time: 0, vpu_time: 452, compute_time: 452, memory_time: 1590, ici_time: 0, vmem_time: 218.22819967830884, exe_time: 1590\n",
      "Calculated vmem_time_ns: 18884.696068857233 for op: AttentionservingdecodeSoftmaxQKVMatMulattnavgprefix2QKprefix2Vcache2\n",
      "Op: AttentionservingdecodeSoftmaxQKVMatMulattnavgprefix2QKprefix2Vcache2, mxu_time: 77120, vpu_time: 12850, compute_time: 77120, memory_time: 91134, ici_time: 0, vmem_time: 18884.696068857233, exe_time: 91134\n",
      "Calculated vmem_time_ns: 2364.098699831495 for op: AttentionservingdecodeSoftmaxQKVMatMulattnavgsuffix2QKsuffix2Vsuffix2\n",
      "Op: AttentionservingdecodeSoftmaxQKVMatMulattnavgsuffix2QKsuffix2Vsuffix2, mxu_time: 9657, vpu_time: 1606, compute_time: 9657, memory_time: 11402, ici_time: 0, vmem_time: 2364.098699831495, exe_time: 11402\n",
      "Calculated vmem_time_ns: 33.590628829656865 for op: AttentionservingdecodeSoftmaxQKVattnavgAddBdbeamSNhDBdbeamSNhD\n",
      "Op: AttentionservingdecodeSoftmaxQKVattnavgAddBdbeamSNhDBdbeamSNhD, mxu_time: 0, vpu_time: 2, compute_time: 2, memory_time: 500, ici_time: 0, vmem_time: 33.590628829656865, exe_time: 500\n",
      "Calculated vmem_time_ns: 2359.0027429917277 for op: AttentionservingdecodeAttentionoutputMatMulattnOutput2attnAvg2Wo1\n",
      "Op: AttentionservingdecodeAttentionoutputMatMulattnOutput2attnAvg2Wo1, mxu_time: 9657, vpu_time: 1606, compute_time: 9657, memory_time: 11325, ici_time: 0, vmem_time: 2359.0027429917277, exe_time: 11325\n",
      "Calculated vmem_time_ns: 36.340628829656865 for op: AttentionservingdecodeAttentionlayernormYnormLayerNormy\n",
      "Op: AttentionservingdecodeAttentionlayernormYnormLayerNormy, mxu_time: 0, vpu_time: 13, compute_time: 13, memory_time: 500, ici_time: 0, vmem_time: 36.340628829656865, exe_time: 500\n",
      "Calculated vmem_time_ns: 8246.407738779106 for op: FwdFFNservingdecoderFFgateMatMulhgate2ynorm2WFFgate1\n",
      "Op: FwdFFNservingdecoderFFgateMatMulhgate2ynorm2WFFgate1, mxu_time: 33751, vpu_time: 5622, compute_time: 33751, memory_time: 39607, ici_time: 0, vmem_time: 8246.407738779106, exe_time: 39607\n",
      "Calculated vmem_time_ns: 8246.407738779106 for op: FwdFFNservingdecoderFFupMatMulhup2ynorm2WFFup1\n",
      "Op: FwdFFNservingdecoderFFupMatMulhup2ynorm2WFFup1, mxu_time: 33751, vpu_time: 5622, compute_time: 33751, memory_time: 39607, ici_time: 0, vmem_time: 8246.407738779106, exe_time: 39607\n",
      "Calculated vmem_time_ns: 34.590628829656865 for op: FwdFFNservingdecoderFFgateuphgateup2hgate2hup1\n",
      "Op: FwdFFNservingdecoderFFgateuphgateup2hgate2hup1, mxu_time: 0, vpu_time: 6, compute_time: 6, memory_time: 500, ici_time: 0, vmem_time: 34.590628829656865, exe_time: 500\n",
      "Calculated vmem_time_ns: 8246.407738779106 for op: FwdFFNservingdecoderFFoutputMatMulffdown2hgateup2WFFdown1\n",
      "Op: FwdFFNservingdecoderFFoutputMatMulffdown2hgateup2WFFdown1, mxu_time: 33751, vpu_time: 5622, compute_time: 33751, memory_time: 39607, ici_time: 0, vmem_time: 8246.407738779106, exe_time: 39607\n",
      "Calculated vmem_time_ns: 33.590628829656865 for op: FFNservingdecoderAttnPlusFFnattnPlusFFnAddBdbeamSMmhBdbeamSMmh\n",
      "Op: FFNservingdecoderAttnPlusFFnattnPlusFFnAddBdbeamSMmhBdbeamSMmh, mxu_time: 0, vpu_time: 2, compute_time: 2, memory_time: 500, ici_time: 0, vmem_time: 33.590628829656865, exe_time: 500\n",
      "Calculated vmem_time_ns: 0 for op: DecodeSendOutputToPipelineICIsendOutputICITransfer4096\n",
      "Op: DecodeSendOutputToPipelineICIsendOutputICITransfer4096, mxu_time: 0, vpu_time: 0, compute_time: 0, memory_time: 500, ici_time: 3330, vmem_time: 0, exe_time: 3330\n"
     ]
    }
   ],
   "source": [
    "from neusim.npusim.frontend.llm_ops_generator import LLMOpsGenerator\n",
    "\n",
    "# Create an instance of the ops generator\n",
    "ops_generator = LLMOpsGenerator(config)\n",
    "# Alternatively, this can be done using a dictionary directly:\n",
    "#   ops_generator = LLMOpsGenerator(config_dict)\n",
    "# For the LLMOpsGenerator, we support both training and inference modes.\n",
    "# The class LLMOpsGeneratorInference is just an alias of LLMOpsGenerator.\n",
    "# It will gnerate the forward pass for both prefill and decode.\n",
    "# The class LLMOpsGeneratorTraining is used for training.\n",
    "# It will generate the forward and backward passes.\n",
    "\n",
    "# Simulate the prefill and decode for LLM inference.\n",
    "# ops is a list of all operators (both prefill and decode).\n",
    "# Note that the returned lists are shallow copies and share the underlying Operator objects.\n",
    "ops, prefill_ops, decode_ops = ops_generator.generate(dump_to_file=True, separate_prefill_decode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6bf39",
   "metadata": {},
   "source": [
    "The simulation results will be dumped as CSV files in the specified output path.\n",
    "As we specified `separate_prefill_decode=True`, the ops generator will generate three CSV files:\n",
    "- `llama3-8b-inference-v5p.csv`: Contains the operators for both prefill and decode phases.\n",
    "- `llama3-8b-inference-v5p_prefill.csv`: Contains only the operators for the prefill phase.\n",
    "- `llama3-8b-inference-v5p_decode.csv`: Contains only the operators for the decode phase.\n",
    "\n",
    "The generated ops and their statistics are also returned as a list of `Operator` objects (see [Operator.py](../npusim/frontend/Operator.py)). They can be programmatically accessed for further analysis or visualization in your own script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070f48f",
   "metadata": {},
   "source": [
    "### Power Simulation\n",
    "\n",
    "The `power_analysis_lib.py` module implements the per-operator energy consumption analysis.\n",
    "The power simulation can be invoked as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e28fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from neusim.npusim.frontend import power_analysis_lib as power_lib\n",
    "\n",
    "# This can be either a power_lib.PowerGatingConfig or a string representing a pre-defined power gating strategy. See the get_power_gating_config() function in power_analysis_lib.py for more details.\n",
    "power_gating_strategy = \"NoPG\"  # NoPG stands for no power gating.\n",
    "\n",
    "# This can also be done separately for prefill_ops and decode_ops.\n",
    "for op in ops:\n",
    "    power_lib.analyze_operator_energy(\n",
    "        op, config, pg_config=power_gating_strategy\n",
    "    )\n",
    "\n",
    "# convert the Operator objects to dictionaries for CSV writing\n",
    "ops = [op.to_csv_dict() for op in ops]\n",
    "\n",
    "# Dump the operators into a CSV file.\n",
    "with open(config.output_file_path, \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=ops[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e681a",
   "metadata": {},
   "source": [
    "## Supported Ops Generators\n",
    "\n",
    "We currently provide the following ops generator modules for different model architectures:\n",
    "- [`llm_ops_generator.py`](llm_ops_generator.py): Contains `LLMOpsGeneratorInference` and `LLMOpsGeneratorTraining` for Llama architecture, and `DeepSeekOpsGenerator` for DeepSeek inference.\n",
    "- [`dlrm_ops_generator.py`](dlrm_ops_generator.py): Contains `DLRMOpsGenerator` for DLRM inference.\n",
    "- [`dit_ops_generator.py`](dit_ops_generator.py): Contains `DiTOpsGenerator` for DiT inference. This is a wrapper of LLMOpsGenerator since DiT inference is similar to LLM prefill.\n",
    "- [`gligen_ops_generator.py`](gligen_ops_generator.py): Contains `GLIGENOpsGenerator` for GLIGEN inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea173a",
   "metadata": {},
   "source": [
    "## Pre-defined Configurations\n",
    "\n",
    "See [`configs/`](configs/) for the pre-defined configurations for different model architectures and NPU platforms.\n",
    "- [`configs/chips/`](configs/chips/): Contains the NPU configurations for different NPU platforms. The configurable parameters are defined in [`ChipConfig.py`](configs/chips/ChipConfig.py).\n",
    "- [`configs/models/`](configs/models/): Contains the model configurations for different model architectures. We create different pydantic classes for different model architectures as they require customized configurations. Parameters that are common to all model architectures are defined in [`ModelConfig.py`](configs/models/ModelConfig.py). Model-specific parameters are defined in their respective classes, such as [`LLMConfig.py`](configs/models/LLMConfig.py) for LLMs, [`DLRMConfig.py`](configs/models/DLRMConfig.py) for DLRM, [`DiTConfig.py`](configs/models/DiTConfig.py) for DiT, and [`GLIGENConfig.py`](configs/models/GLIGENConfig.py) for GLIGEN.\n",
    "- [`configs/systems/`](configs/systems/): Contains the system configuration. Currently, we only support datacenter power usage efficiency (`PUE`) and carbon intensity `carbon_intensity_kgCO2_per_kWh`. These are used for fleet-wide energy and carbon analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neusim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
